{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7973933,"sourceType":"datasetVersion","datasetId":4692373}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install arff\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.io import arff\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n                \ndata = arff.loadarff(\"/kaggle/input/bank-marketing/phpkIxskf (1).arff\")\ndf = pd.DataFrame(data[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obj = df.select_dtypes(include=(\"object\"))\n\ndecode = obj.apply(lambda x: x.str.decode('utf8'))\n\ndecode.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(columns = [\"V2\",\"V3\",\"V4\",\"V5\",\"V7\",\"V8\",\"V9\",\"V11\",\"V16\",\"Class\"], axis = 1)\n\ndf = pd.concat([df, decode], axis = 1)\n\nnew_order = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'Class']\n\ndf = df.reindex(columns=new_order)\n\ndf.head()","metadata":{"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sütunların yeninden adlandırılması\n\ndf.rename(columns={'V1': 'yas',\n                   'V2': 'is_turu',\n                   'V3': 'medeni_hal',\n                   'V4': 'egitim',\n                   'V5': 'odeme_durumu',\n                   'V6': 'bakiye',\n                   'V7': 'konut_kredi_durum',\n                   'V8': 'bireysel_kredi_durum',\n                   'V9': 'iletisim_turu',\n                   'V10': 'son_iletisim_gunu',\n                   'V11': 'son_iletisim_ayi',\n                   'V12': 'sure_saniye',\n                   'V13': 'kampanya',\n                   'V14': 'gecen_gun_sayisi',\n                   'V15': 'onceki_iletisim_sayisi',\n                   'V16': 'onceki_kampanya_sonucu',\n                   'Class': 'abone_oldu_mu'}, inplace=True)\n\n# İş türü değişkenindeki alt kategorilerin yeniden isimlendirilmesi\n\ndf[\"is_turu\"] = df[\"is_turu\"].replace(\"admin.\", \"admin\")\ndf[\"is_turu\"] = df[\"is_turu\"].replace(\"blue-collar\", \"bluecollar\")\ndf[\"is_turu\"] = df[\"is_turu\"].replace(\"self-employed\", \"selfemployed\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Numerik ve kategorik kolonların seçilmesi\n\nnum_cols = [\"yas\", \"bakiye\", \"son_iletisim_gunu\", \"sure_saniye\",\n            \"kampanya\", \"onceki_iletisim_sayisi\"]\n\ncat_cols = [\"is_turu\", \"medeni_hal\", \"egitim\", \"odeme_durumu\",\n            \"konut_kredi_durum\", \"bireysel_kredi_durum\", \"iletisim_turu\",\n            \"son_iletisim_ayi\", \"onceki_kampanya_sonucu\", \"abone_oldu_mu\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"## UNKNOWN SORUNUNUN ÇÖZÜLMESİ\n\n# İş türü değişkenindeki unknown alt kategorilerinin mod ile doldurulması\n\nmode_value = df[df['is_turu'] != 'unknown']['is_turu'].mode()[0]\ndf['is_turu'].replace('unknown', mode_value, inplace=True)\n\n# Eğitim değişkenindeki unknown alt kategorilerinin mod ile doldurulması\n\nmode_value = df[df['egitim'] != 'unknown']['egitim'].mode()[0]\ndf['egitim'].replace('unknown', mode_value, inplace=True)\n\n# Unknown alt kategorisi yüksek olan değişkenlerin seçilmesi ve unknown alt kategorilerinin silinmesi\n\nhas_unk_cat = [\"onceki_kampanya_sonucu\", \"iletisim_turu\"]\ndf[has_unk_cat] = df[has_unk_cat].replace('unknown', np.nan)\n\n# Silinen değerlerin rastgele doldurulmasıa\n\ndef fill_missing_values_randomly(df, column_name):\n    non_missing_values = df[column_name].dropna().unique()\n    df[column_name] = df[column_name].apply(\n        lambda x: np.random.choice(non_missing_values) if pd.isnull(x) else x\n    )\n\ndef fill_columns_randomly(df):\n    columns_to_fill = has_unk_cat\n    \n    for column in columns_to_fill:\n        fill_missing_values_randomly(df, column)\n\nfill_columns_randomly(df)\n\n## VERİ TÜRETME - EDA İÇİN\n\n# Yaş kategori gruplama\n\ndef yas_aralik(yas):\n    if 18 <= yas <= 30:\n        return \"Genç Yetişkin\"\n    elif 30 < yas <= 65:\n        return \"Yetişkin\"\n    else:\n        return \"Yaşlı\"\n\ndf[\"yas_aralik\"] = df[\"yas\"].apply(yas_aralik)\n\n# Bakiye kategori gruplama\n\nq1 = df['bakiye'].quantile(0.25)\nq2 = df['bakiye'].quantile(0.50)\nq3 = df['bakiye'].quantile(0.75)\n\ndef bakiye_aralik(bakiye):\n    if bakiye <= q1:\n        return \"Düşük Bakiye\" # -8019, 72 arası\n    elif q1 < bakiye <= q2: \n        return \"Orta Bakiye\" # 73, 448 arası\n    elif q2 < bakiye <= q3:\n        return \"Yüksek Bakiye\" # 449, 1428 arası\n    elif q3 < bakiye:\n        return \"Çok Yüksek Bakiye\" # 1429, 102127 arası\n\ndf[\"bakiye_aralik\"] = df[\"bakiye\"].apply(bakiye_aralik)\n\n# Mesleğe göre çalışıyor veya çalışmıyor olarak gruplama\n\ndef calisma_durum(is_turu):\n    if is_turu in [\"unemployed\", \"student\", \"retired\"]:\n        return \"Çalışmıyor\"\n    else:\n        return \"Çalışıyor\"\n    \ndf[\"calisma_durum\"] = df[\"is_turu\"].apply(calisma_durum)\n\n# Gün sayısı -1 ise iletişime geçilmedi -1'den farklı ise iletişime geçildi olarak gruplama\n\ndef iletisim_durum(gecen_gun_sayisi):\n    if gecen_gun_sayisi == -1:\n        return \"İletişime Geçilmedi\"\n    else:\n        return \"İletişime Geçildi\"\n    \ndf[\"iletisim_durum\"] = df[\"gecen_gun_sayisi\"].apply(iletisim_durum)\n\n#cat_cols.extend([\"yas_aralik\", \"bakiye_aralik\", \"calisma_durum\", \"iletisim_durum\"])\n\ndf = df.drop([\"yas_aralik\", \"bakiye_aralik\", \"calisma_durum\", \"iletisim_durum\", \"gecen_gun_sayisi\"], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"code","source":"                                    # ENCODING\n\n#--------------------------------------------------------------------------------------\n\nlabel_encode = [\"egitim\"]\n\nlabel_encoder = LabelEncoder()\n\nfor col in label_encode:\n    df[col] = label_encoder.fit_transform(df[col])\n\n#--------------------------------------------------------------------------------------\n\nis_turu_encode = {'bluecollar':1, 'management':2, 'technician':3, 'admin':4,\n                'services':5, 'retired':6, 'selfemployed':7, 'entrepreneur':8,\n                'unemployed':9, 'housemaid':10, 'student':11}\n\ndf['is_turu'] = df['is_turu'].map(is_turu_encode)\n\n#--------------------------------------------------------------------------------------\n\nmedeni_hal_encode = {'married':1, 'single':2, 'divorced':3}\n\ndf['medeni_hal'] = df['medeni_hal'].map(medeni_hal_encode)\n\n#--------------------------------------------------------------------------------------\n\nodeme_durumu_encode = {'no':1, 'yes':2}\n\ndf['odeme_durumu'] = df['odeme_durumu'].map(odeme_durumu_encode)\n\n#--------------------------------------------------------------------------------------\n\nkonut_kredi_durum_encode = {'no':1, 'yes':2}\n\ndf['konut_kredi_durum'] = df['konut_kredi_durum'].map(konut_kredi_durum_encode)\n\n#--------------------------------------------------------------------------------------\n\nbireysel_kredi_durum_encode = {'no':1, 'yes':2}\n\ndf['bireysel_kredi_durum'] = df['bireysel_kredi_durum'].map(bireysel_kredi_durum_encode)\n\n#--------------------------------------------------------------------------------------\n\niletisim_turu_encode = {'cellular':1, 'telephone':2}\n\ndf['iletisim_turu'] = df['iletisim_turu'].map(iletisim_turu_encode)\n\n#--------------------------------------------------------------------------------------\n\nson_iletisim_ayi_encode = {'jan':1, 'feb':2, 'mar':3, 'apr':4,\n                           'may':5, 'jun':6, 'jul':7, 'aug':8,\n                           'sep':9, 'oct':10, 'nov':11, 'dec':12}\n\ndf['son_iletisim_ayi'] = df['son_iletisim_ayi'].map(son_iletisim_ayi_encode)\n\n#--------------------------------------------------------------------------------------\n\nonceki_kampanya_sonucu_encode = {'other':1, 'failure':2, 'success':3}\n\ndf['onceki_kampanya_sonucu'] = df['onceki_kampanya_sonucu'].map(onceki_kampanya_sonucu_encode)\n\n#--------------------------------------------------------------------------------------\n\n#abone_oldu_mu? --> 'no':1, 'yes':2\n\n#--------------------------------------------------------------------------------------\n\n#yas_aralik_encode = {'Genç Yetişkin':1, 'Yetişkin':2, 'Yaşlı':3}\n\n#df['yas_aralik'] = df['yas_aralik'].map(yas_aralik_encode)\n\n#--------------------------------------------------------------------------------------\n\n#calisma_durum_encode = {'Çalışmıyor':1, 'Çalışıyor':2}\n\n#df['calisma_durum'] = df['calisma_durum'].map(calisma_durum_encode)\n\n#--------------------------------------------------------------------------------------\n\n#iletisim_durum_encode = {'İletişime Geçilmedi':1, 'İletişime Geçildi':2}\n\n#df['iletisim_durum'] = df['iletisim_durum'].map(iletisim_durum_encode)\n\n#--------------------------------------------------------------------------------------","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculation of the Spearman correlation\n#target = 'abone_oldu_mu'\n#df_ordered = pd.concat([df.drop(target,axis=1), df[target]],axis=1)\n#corr = df_ordered.corr(method='spearman')\n\n# Create a mask so that we see the correlation values only once\n#mask = np.zeros_like(corr)\n#mask[np.triu_indices_from(mask,1)] = True\n\n# Plot the heatmap correlation\n#plt.figure(figsize=(12,8), dpi=80)\n#sns.heatmap(corr, mask=mask, annot=True,fmt='.2f', linewidths=0.2)\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Aykırı değer probleminin çözülmesi\n\n#bounds = {}\n#for col in num_cols:\n#    q1 = df[col].quantile(0.05)\n#    q3 = df[col].quantile(0.95)\n#    iqr = q3 - q1\n#    lower_bound = q1 - 1.5 * iqr\n#    upper_bound = q3 + 1.5 * iqr\n#    bounds[col] = {'lower_bound': lower_bound, 'upper_bound': upper_bound}\n\n## Her bir sayısal değişken için belirli aralık içindeki değerleri seçelim\n##selected_values = {}\n##for col in num_cols:\n##    selected_values[col] = df[col][(df[col] >= bounds[col]['lower_bound']) & (df[col] <= bounds[col]['upper_bound'])]\n    \n# Belirli aralık dışındaki değerleri çıkaralım\n#for col in num_cols:\n#    df = df[(df[col] >= bounds[col]['lower_bound']) & (df[col] <= bounds[col]['upper_bound'])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"abone_oldu_mu\"] = df[\"abone_oldu_mu\"].astype(int)\n\nua_gozlemler_secili = df[df['abone_oldu_mu'] == 1].sample(n=20000, random_state=42)\n\ndf = pd.concat([ua_gozlemler_secili, df[df['abone_oldu_mu'] != 1]])\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df[\"abone_oldu_mu\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ny = df[\"abone_oldu_mu\"]                  \nX = df.drop([\"abone_oldu_mu\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify = y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Etiketleri integer türüne dönüştür\n# y_train = y_train.astype(int)\n# y_test = y_test.astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardization - Standart Scaler\n\n#continuous_cols = num_cols\n\n#scaler = StandardScaler()\n\n#X_train[continuous_cols] = scaler.fit_transform(X_train[continuous_cols])\n#X_test[continuous_cols] = scaler.transform(X_test[continuous_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Standardization - Robust Scaler\n\ncontinuous_cols = num_cols\n\nscaler2 = RobustScaler()\n\nX_train[continuous_cols] = scaler2.fit_transform(X_train[continuous_cols])\nX_test[continuous_cols] = scaler2.transform(X_test[continuous_cols])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rastgele Orman(RF)","metadata":{}},{"cell_type":"code","source":"def model_evaluation(classifier):\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test,classifier.predict(X_test))\n    names = ['True Neg','False Pos','False Neg','True Pos']\n    counts = [value for value in cm.flatten()]\n    percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]\n    labels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in zip(names,counts,percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cm,annot = labels,fmt ='')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - Without Sampling\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=100)\nrfc.fit(X_train, y_train)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_Without_Sampling']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - Random Over Sampling\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Oversampling\n\nprint(\"Before oversampling: \",Counter(y_train))\nRandomOverSampler = RandomOverSampler(random_state=42)\nX_train_oversampler,y_train_oversampler = RandomOverSampler.fit_resample(X_train,y_train)\nprint(\"After oversampling: \",Counter(y_train_oversampler))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=100)\nrfc.fit(X_train_oversampler, y_train_oversampler)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_Random_Over_Sampling']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - Smote\n\nsampling_strategy = {1: 16000, 2: 13000}\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Oversampling\n\nprint(\"Before oversampling: \",Counter(y_train))\nSMOTE = SMOTE(random_state=42, sampling_strategy=sampling_strategy)\nX_train_smote,y_train_smote= SMOTE.fit_resample(X_train,y_train)\nprint(\"After oversampling: \",Counter(y_train_smote))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=100, class_weight = 'balanced')\nrfc.fit(X_train_smote, y_train_smote)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_Smote']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - BorderLine Smote\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Oversampling\n\nsampling_strategy = {1: 16000, 2: 13000}\n\nprint(\"Before oversampling: \",Counter(y_train))\nBorderlineSMOTE = BorderlineSMOTE(random_state=42, sampling_strategy = sampling_strategy)\nX_train_border,y_train_border= BorderlineSMOTE.fit_resample(X_train,y_train)\nprint(\"After oversampling: \",Counter(y_train_border))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(n_estimators= 488, max_features='auto', max_depth= 101, min_samples_split=2, min_samples_leaf= 1, random_state=42)\nrfc.fit(X_train_border, y_train_border)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_BorderLine_Smote']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - Random Under Sampling\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Undersampling\n\nprint(\"Before undersampling: \",Counter(y_train))\nRandomUnderSampler = RandomUnderSampler(random_state=42)\nX_train_undersamp,y_train_undersamp= RandomUnderSampler.fit_resample(X_train,y_train)\nprint(\"After undersampling: \",Counter(y_train_undersamp))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=100)\nrfc.fit(X_train_undersamp, y_train_undersamp)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_Random_Under_Sampling']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - TomekLinks\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Undersampling\n\nprint(\"Before undersampling: \",Counter(y_train))\nTomekLinks = TomekLinks()\nX_train_tomek, y_train_tomek = TomekLinks.fit_resample(X_train,y_train)\nprint(\"After undersampling: \",Counter(y_train_tomek))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - -\")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=100)\nrfc.fit(X_train_tomek, y_train_tomek)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_TomekLinks']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Tunning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import accuracy_score\n\nRandomOverSampler = RandomOverSampler(random_state=42)\nX_train_oversampler,y_train_oversampler = RandomOverSampler.fit_resample(X_train,y_train)\n\ndef objective(trial):\n    # Number of trees in random forest\n    n_estimators = trial.suggest_int(name=\"n_estimators\", low=100, high=500)\n\n    # Maximum number of levels in tree\n    max_depth = trial.suggest_int(name=\"max_depth\", low=1, high=50)\n\n    # Minimum number of samples required to split a node\n    min_samples_split = trial.suggest_int(name=\"min_samples_split\", low=2, high=20)\n\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = trial.suggest_int(name=\"min_samples_leaf\", low=1, high=10)\n    \n    params = {\n        \"n_estimators\": n_estimators,\n        \"max_depth\": max_depth,\n        \"min_samples_split\": min_samples_split,\n        \"min_samples_leaf\": min_samples_leaf\n    }\n    model = RandomForestClassifier(random_state=42, **params)\n    \n    cv_score = cross_val_score(model, X_train_oversampler, y_train_oversampler, n_jobs=-1, cv=5, scoring='f1')\n    mean_cv_f1 = cv_score.mean()\n    return mean_cv_f1\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=100)\n\nprint(study.best_params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Random Forest - Random Over Sampling\n\nfrom sklearn.model_selection import cross_val_score\n\naccuracy= []\nrecall =[]\nroc_auc= []\nprecision = []\naucpr = []\nf1 = []\n\n# Oversampling\n\nprint(\"Before oversampling: \",Counter(y_train))\nRandomOverSampler = RandomOverSampler(random_state=42)\nX_train_oversampler,y_train_oversampler = RandomOverSampler.fit_resample(X_train,y_train)\nprint(\"After oversampling: \",Counter(y_train_oversampler))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\n# Model Eğitim - Tahmin\n\nrfc = RandomForestClassifier(random_state=42, n_estimators=222, max_depth = 38, min_samples_split = 2, min_samples_leaf = 1)\n\n# Cross-validation ile modelin performansını değerlendirin\ntrain_cv_scores = cross_val_score(rfc, X_train_oversampler, y_train_oversampler, cv=5, scoring='f1')\ntest_cv_scores = cross_val_score(rfc, X_test, y_test, cv=5, scoring='f1')\n\n# Cross-validation sonuçlarını yazdırın\nprint(\"Train Cross-Validation Scores:\", train_cv_scores)\nprint(\"Test Cross-Validation Scores:\", test_cv_scores)\n\nrfc.fit(X_train_oversampler, y_train_oversampler)\n\ny_pred = rfc.predict(X_test)\ny_pred_prob = rfc.predict_proba(X_test)[:, 1]\n\n# Model Metrikleri\n\nprint(classification_report(y_test, y_pred))\nprint(\"- - - - - - - - - - - - - - - - - - - - - - - - - \")\n\nprecision.append(round(precision_score(y_test, y_pred, pos_label=2),4))\nrecall.append(round(recall_score(y_test, y_pred, pos_label=2),4))\nf1.append(round(f1_score(y_test, y_pred, pos_label=2),4))\naucpr.append(round(average_precision_score(y_test, y_pred_prob, pos_label=2),4))\nroc_auc.append(round(roc_auc_score(y_test, y_pred),4))\n\nmodel_names = ['RF_Random_Over_Sampling']\n\nresult_model = pd.DataFrame({'Precision':precision, 'Recall':recall, 'F1-Score':f1, 'Roc_Auc':roc_auc, 'Aucpr':aucpr}, index=model_names)\n\nresult_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"importances = rfc.feature_importances_\nfeature_names = df.drop('abone_oldu_mu', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances of the forest\ndef feature_importance_graph(indices, importances, feature_names):\n    plt.figure(figsize=(8,8))\n    plt.title(\" Feature importances \\n with Random Forest Classifier\", fontsize=18)\n    plt.barh(range(len(indices)), importances[indices],  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])\n\nfeature_importance_graph(indices, importances, feature_names)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import tree\n\nfor i in range(len(rfc.estimators_)):\n    fig = plt.figure(figsize=(150, 150))\n    _ = tree.plot_tree(rfc.estimators_[i], feature_names=feature_names, class_names=['No', 'Yes'], filled=True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
